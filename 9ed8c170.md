---
date: 2021-02-22T23:38
tags: 
  - stub
---

# What are the difficulties the tokenizer faces with languages?

German nouns not segmented

Lebensversicherungsgesellschaftsangetsellter

We use [compound splitter module](https://pypi.org/project/compound-split/).

Gives us a 15% performance boost.

Japanese and chinese words have no spaces between words.

Arabic / Hebrew written right to left, certain items are written left to right.

L'ensemble -> one / two tokens??
